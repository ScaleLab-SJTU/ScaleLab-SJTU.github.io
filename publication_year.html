<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
        
        <meta name="keywords" content="" />
        
        <!-- Page Title -->
        <title>Publications | Wenhao Chai</title>

        <!-- Favicons -->
        <link rel="apple-touch-icon" sizes="180x180" href="assets/img/favicons/apple-touch-icon.png">
        <link rel="icon" type="image/png" sizes="32x32" href="assets/img/favicons/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="assets/img/favicons/favicon-16x16.png">
        <link rel="manifest" href="assets/img/favicons/site.webmanifest">
        <link rel="mask-icon" href="assets/img/favicons/safari-pinned-tab.svg" color="#f23838">
        <meta name="msapplication-TileColor" content="#da532c">
        <meta name="theme-color" content="#ffffff">

        <!-- Vendor Stylesheets -->
        <link href="https://fonts.googleapis.com/css?family=Oswald:300,400,500,700%7CRoboto:300,400,700" rel="stylesheet">
        <link href="assets/vendor/material-design-iconic-font/dist/css/material-design-iconic-font.min.css" rel="stylesheet">
        <link href="assets/vendor/jquery.mb.vimeo_player/dist/css/jquery.mb.vimeo_player.min.css" rel="stylesheet">
        <link href="assets/vendor/@fortawesome/fontawesome-free/css/all.css" rel="stylesheet">

        <!-- Theme Stylesheets -->
        <link href="assets/css/theme.css" rel="stylesheet">

        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-22940424-1"></script>
        <script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());

            gtag('config', 'UA-22940424-1');
        </script>
        <style>
          a
          {
              text-decoration: none;

              color: #6c757d;
              background-color: transparent;
          }
          a:hover
          {
              text-decoration: underline; 

              color: #c00000;
          }

          a:not([href]):not([class])
          {
              text-decoration: none; 

              color: inherit;
          }
          a:not([href]):not([class]):hover
          {
              text-decoration: none; 

              color: inherit;
          }
        </style>
    </head>

    <body>
        <!-- Preloader -->
        <div class="preloader">
            <div class="spinner">
                <div class="circles"></div>
            </div>
        </div>
        <!-- End of Preloader -->

        <!-- Header -->
        <header class="spyre-navbar navbar navbar-expand-lg bg-dark navbar-dark fixed-top align-items-center" data-transparent data-text-color="#ffffff">
            <div class="container">
                <a class="navbar-brand mr-lg-5 mr-xl-7" href="index.html">
                    <img src="assets/img/logo.png" class="d-none d-lg-block" alt="MMLab" width="183" />
                    <img src="assets/img/logo.png" class="d-block d-lg-none" alt="MMLab" width="150" />
                </a>

                <!-— Desktop Menu -->
                <div class="collapse navbar-collapse" id="navbarSupportedContent">
                    <ul class="navbar-nav mr-auto">
                        <li class="pl-2 nav-item navbar-text"><a href="index.html" class="nav-link text-400">Home</a></li>
                        <!-- <li class="pl-5 nav-item navbar-text"><a href="team.html" class="nav-link text-400">Team</a></li> -->
                        <li class="pl-5 nav-item navbar-text"><a href="research.html" class="nav-link text-400">Research</a></li>
                        <li class="pl-5 nav-item navbar-text"><a href="publication_topic.html" class="nav-link">Publications</a></li>
                        <li class="pl-5 nav-item navbar-text"><a href="downloads.html" class="nav-link text-400">Code | Datasets</a></li>
                        <!-- <li class="pl-5 nav-item navbar-text"><a href="careers.html" class="nav-link text-400">Join Us</a></li> -->
                    </ul>
                </div>
                <!-— End of Desktop Menu -->

                <div class="menu-toggle d-block d-lg-none">
                    <div class="hamburger">
                        <span></span>
                        <span></span>
                        <span></span>
                    </div>
                    <div class="cross">
                        <span></span>
                        <span></span>
                    </div>
                </div>
            </div>

            <!-- Spyrenav Overlay -->
            <div class="spyre-navbar-overlay overlay-slide">
                <div class="container">
                    <div class="row">
                        <div class="spyre-navbar-nav-container col-md-6 col-lg-5 col-xl-4 bg-white ext-l">
                            <nav class="spyre-navbar-nav">
                                <ul class="spyre-nav">
                                    <li class="spyre-nav-item"><a href="index.html" class="spyre-nav-link">Home</a></li>
                                    <li class="spyre-nav-item"><a href="research.html" class="spyre-nav-link">Our Research</a></li>
                                    <!-- <li class="spyre-nav-item"><a href="team.html" class="spyre-nav-link">Team</a></li> -->
                                    <li class="spyre-nav-item dropdown">
                                        <a href="publication_year.html#" class="spyre-nav-link dropdown-toggle" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Publications</a>
                                        <ul class="dropdown-menu" aria-labelledby="navbarDropdown">
                                            <li class="dropdown-menu-item"><a href="publication_topic.html" class="dropdown-menu-link">By Topic</a></li>
                                            <li class="dropdown-menu-item"><a href="publication_year.html" class="dropdown-menu-link">By Year</a></li>
                                        </ul>
                                    </li>
                                    <li class="spyre-nav-item"><a href="downloads.html" class="spyre-nav-link">Code and Datasets</a></li>
                                    <!-- <li class="spyre-nav-item"><a href="careers.html" class="spyre-nav-link">Join Us</a></li> -->
                                </ul>
                            </nav>
                        </div>
        
                        <div class="col-lg-7 col-xl-8 d-none d-md-block">
                            <div class="d-flex flex-column h-100">
                                <div class="d-flex h-100">
                                    <div class="align-self-center">
                                        <div class="text-uppercase"
                                            data-background-text="computer vision"
                                            data-color="#7079a2"
                                            data-opacity="0.02"
                                            data-font-size="85px"
                                            data-font-weight="500"
                                            data-offset-x="-5%"
                                            data-letter-spacing="5px"
                                        ></div>
                                        <div class="text-uppercase"
                                            data-background-text="mmlab"
                                            data-color="#7079a2"
                                            data-opacity="0.04"
                                            data-font-size="175px"
                                            data-font-weight="500"
                                            data-offset-x="29%"
                                            data-padding="7vh 0 2vh 0"
                                            data-letter-spacing="5px"
                                        ></div>
                                        <div class="text-uppercase"
                                            data-background-text="deep learning"
                                            data-color="#7079a2"
                                            data-opacity="0.03"
                                            data-font-size="140px"
                                            data-font-weight="500"
                                            data-offset-x="15%"
                                            data-letter-spacing="5px"
                                        ></div>
                                    </div>
                                </div>
                                
                                <div class="mt-auto">
                                    <ul class="nav flex-nowrap float-right">
                                        <li class="nav-item">
                                            <a class="nav-link px-2" href="https://twitter.com/re5e1f" target="_blank">
                                                <i class="zmdi zmdi-twitter text-white"></i>
                                            </a>
                                        </li>
                                    </ul>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            <!-- End of Spyrenav Overlay -->
        </header>
        <!-- End of Header -->

        <!-- Main Content -->
        <main class="main minh-100vh">
            <!-- Section -->
            <section class="py-0 overflow-hidden text-center">
                <div class="bg-container overlay overlay-dark-60 parallax" data-rellax-percentage="0.5" style="background-image: url(assets/img/backgrounds/bg-03.jpg)">
                </div>
                <span id="2024"></span>
            </section>
            <!-- End of Section -->

            <!-- Section -->
            <section id="section-1" class="pb-0">
                <div class="container">
                    <div class="row">
                        <div class="col-lg-4 order-lg-1">
                            <div class="pb-6 pt-6 py-lg-3" data-toggle="sticky" data-sticky-offset-top="100">
                                <h5 class="mb-4 text-uppercase text-600">Years</h5>
                                <ul class="mb-5 mb-lg-6 pl-4 text-600">
                                    <li class="mb-1"><a href="publication_year.html#2024" class="text-600">2024</a></li>
                                    <li class="mb-1"><a href="publication_year.html#2023" class="text-600">2023</a></li>
                                    <li class="mb-1"><a href="publication_year.html#2022" class="text-600">2022</a></li>
                                </ul>

                                <h5 class="mb-4 text-uppercase text-600">Sort by</h5>
                                <ul class="mb-5 mb-lg-6 pl-4 text-600">
                                    <li class="mb-1"><a href="publication_topic.html#section-1" class="text-600">Topics</a></li>
                                </ul>
                                
                                <!-- <h5 class="mb-4 text-uppercase text-600">Highlights</h5>
                                <ul class="mb-5 mb-lg-6 pl-4 text-600">
                                    <li class="mb-1"><a href="conference/neurips2023/index.html" class="text-600">NeurIPS 2023</a></li>
                                    <li class="mb-1"><a href="conference/iccv2023/index.html" class="text-600">ICCV 2023</a></li>
                                    <li class="mb-1"><a href="conference/cvpr2023/index.html" class="text-600">CVPR 2023</a></li>
                                </ul> -->
                            </div>
                        </div>

                        <div class="col-lg-8 order-lg-2 pb-6 pb-lg-0 pt-lg-3">
                            <div class="mb-8">
                                <h6>* Equal contribution. <sup>&dagger;</sup> Project lead. <sup>&Dagger;</sup> Corresponding author.</h6>
                                <br>

                                <h2 class="mb-4 text-uppercase">2024</h2>

                                <h4 class="mb-4 text-uppercase">Conference</h4>
                                <ol>
                                    <li>
                                      <span class="text-primary"><b>MovieChat: From Dense Token to Sparse Memory in Long Video Understanding</b></span> 
                                      <br />
                                      <span class="text-500">
                                        Enxin Song*, <b>Wenhao Chai</b>*<sup>&dagger;</sup>, Guanhong Wang*, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo, Tian Ye, Yanting Zhang, Yan Lu, Jenq-Neng Hwang, Gaoang Wang<sup>&Dagger;</sup> <br /> 
                                        Computer Vision and Pattern Recognition (CVPR), 2024<br /> 
                                      </span>
                                      <a href="https://rese1f.github.io/MovieChat/">[Website]</a>
                                      <a href="https://arxiv.org/abs/2307.16449">[Paper]</a>
                                      <a href="https://www.youtube.com/watch?v=rNzCzflVqBU">[Video]</a>
                                      <a href="https://huggingface.co/datasets/Enxin/MovieChat-1K-test">[Dataset]</a>
                                      <a href="https://github.com/rese1f/MovieChat">[Code]</a>
                                    </li>
                                    <br>
                                    <li>
                                      <span class="text-primary"><b>Learning Diffusion Texture Priors for Image Restoration</b></span> 
                                      <br />
                                      <span class="text-500">
                                        Tian Ye, Sixiang Chen, <b>Wenhao Chai</b>, Zhaohu Xing, Jing Qin, Ge Lin, Lei Zhu<sup>&Dagger;</sup> <br /> 
                                        Computer Vision and Pattern Recognition (CVPR), 2024 <span style="color: orange;">(Highlight)</span> <br /> 
                                      </span>
                                      <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Ye_Learning_Diffusion_Texture_Priors_for_Image_Restoration_CVPR_2024_paper.html">[Paper]</a>
                                    </li>
                                    <br>
                                    <li>
                                      <span class="text-primary"><b>See and Think: Embodied Agent in Virtual Environment</b></span> 
                                      <br />
                                      <span class="text-500">
                                        Zhonghan Zhao*, <b>Wenhao Chai</b>*<sup>&dagger;</sup>, Xuan Wang*, Boyi Li, Shengyu Hao, Shidong Cao, Tian Ye, Jenq-Neng Hwang, Gaoang Wang<sup>&Dagger;</sup> <br /> 
                                        European Conference on Computer Vision (ECCV), 2024<br /> 
                                      </span>
                                      <a href="https://rese1f.github.io/STEVE/">[Website]</a>
                                      <a href="https://arxiv.org/abs/2311.15209">[Paper]</a>
                                      <a href="https://github.com/rese1f/STEVE">[Dataset]</a>
                                      <a href="https://github.com/rese1f/STEVE">[Code]</a>
                                    </li>
                                    <br>
                                    <li>
                                      <span class="text-primary"><b>RT-Pose: A 4D Radar Tensor-based 3D Human Pose Estimation and Localization Benchmark</b></span> 
                                      <br />
                                      <span class="text-500">
                                      Yuan-Hao Ho, Jen-Hao Cheng, Sheng Yao Kuan, Zhongyu Jiang, <b>Wenhao Chai</b>, Hsiang-Wei Huang, Jenq-Neng Hwang, Chih-Lung Lin<sup>&Dagger;</sup> <br /> 
                                      European Conference on Computer Vision (ECCV), 2024<br /> 
                                      </span>
                                      <a href="https://arxiv.org/abs/2407.13930">[Paper]</a>
                                      <a href="https://huggingface.co/datasets/uwipl/RT-Pose">[Dataset]</a>
                                      <a href="https://github.com/ipl-uw/RT-POSE">[Code]</a>
                                    </li>
                                    <br>
                                    <li>
                                        <span class="text-primary"><b>UniAP: Towards Universal Animal Perception in Vision via Few-shot Learning</b></span> 
                                        <br />
                                        <span class="text-500">
                                        Meiqi Sun*, Zhonghan Zhao*, <b>Wenhao Chai</b>*, Hanjun Luo, Shidong Cao, Yanting Zhang, Jenq-Neng Hwang, Gaoang Wang<sup>&Dagger;</sup> <br /> 
                                        Association for the Advancement of Artificial Intelligence (AAAI), 2024<br /> 
                                        </span>
                                        <a href="https://rese1f.github.io/UniAP/">[Website]</a>
                                        <a href="https://arxiv.org/abs/2308.09953">[Paper]</a>
                                        <a href="https://github.com/rese1f/UniAP">[Code]</a>
                                    </li>
                                    <br>
                                    <li>
                                      <span class="text-primary"><b>Ego3DT: Tracking Every 3D Object in Ego-centric Videos</b></span>
                                      <br />
                                      <span class="text-500">
                                        Shengyu Hao*, <b>Wenhao Chai</b>*, Zhonghan Zhao*, Meiqi Sun, Wendi Hu, Jieyang Zhou, Yixian Zhao, Qi Li, Yizhou Wang, Xi Li, Gaoang Wang<sup>&Dagger;</sup> <br />
                                        ACM International Conference on Multimedia (ACM MM), 2024<br />
                                      </span>
                                      <a href="https://arxiv.org/abs/2410.08530">[Paper]</a>
                                    </li>
                                    <br>
                                    <li>
                                      <span class="text-primary"><b>LLaVA-Ultra: Large Chinese Language and Vision Assistant for Ultrasound</b></span>
                                      <br />
                                      <span class="text-500">
                                        Xuechen Guo, <b>Wenhao Chai</b>, Shi-Yan Li, Gaoang Wang<sup>&Dagger;</sup> <br />
                                        ACM International Conference on Multimedia (ACM MM), 2024<br />
                                      </span>
                                      <a href="https://openreview.net/forum?id=7ZYEoB71Vd">[Paper]</a>
                                    </li>
                                    <br>
                                    <li>
                                        <span class="text-primary"><b>Blind Inpainting with Object-aware Discrimination for Artificial Marker Removal</b></span>
                                        <br />
                                        <span class="text-500">
                                          Xuechen Guo, Wenhao Hu, Chiming Ni, <b>Wenhao Chai</b>, Shiyan Li, Gaoang Wang<sup>&Dagger;</sup> <br />
                                          International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2024<br />
                                        </span>
                                        <a href="https://arxiv.org/abs/2303.15124">[Paper]</a>
                                    </li>
                                    <br>
                                    <li>
                                        <span class="text-primary"><b>Back to Optimization: Diffusion-based Zero-Shot 3D Human Pose Estimation</b></span> 
                                        <br />
                                        <span class="text-500">
                                          Zhongyu Jiang, Zhuoran Zhou, Lei Li, <b>Wenhao Chai</b>, Cheng-Yen Yang, Jenq-Neng Hwang<sup>&Dagger;</sup> <br /> 
                                          Winter Conference on Applications of Computer Vision (WACV), 2024<br /> 
                                        </span>
                                        <a href="https://zhyjiang.github.io/ZeDO-proj/">[Website]</a>
                                        <a href="https://arxiv.org/abs/2307.03833">[Paper]</a>
                                        <a href="https://github.com/ipl-uw/ZeDO-Release">[Code]</a>
                                      </li>
                                      <br>
                                      <li>
                                        <span class="text-primary"><b>STEVE Series: Step-by-Step Construction of Agent Systems in Minecraft</b></span> 
                                        <br />
                                        <span class="text-500">
                                          Zhonghan Zhao*, <b>Wenhao Chai</b>*<sup>&dagger;</sup>, Xuan Wang, Ke Ma, Kewei Chen, Dongxu Guo, Tian Ye, Yanting Zhang, Hongwei Wang, Gaoang Wang<sup>&Dagger;</sup> <br /> 
                                          Computer Vision and Pattern Recognition Workshop (CVPRW), 2024<br />
                                        </span>
                                        <a href="https://rese1f.github.io/STEVE/">[Website]</a>
                                        <a href="https://arxiv.org/abs/2406.11247">[Paper]</a>
                                        <a href="https://github.com/rese1f/STEVE">[Dataset]</a>
                                        <a href="https://github.com/rese1f/STEVE">[Code]</a>
                                      </li>
                                      <br>
                                      <li>
                                        <span class="text-primary"><b>Hierarchical Auto-Organizing System for Open-Ended Multi-Agent Navigation</b></span> 
                                        <br />
                                        <span class="text-500">
                                          Zhonghan Zhao*, Kewei Chen*, Dongxu Guo*, <b>Wenhao Chai</b><sup>&dagger;</sup>, Tian Ye, Yanting Zhang, Gaoang Wang<sup>&Dagger;</sup> <br /> 
                                          International Conference on Learning Representations Workshop (ICLRW), 2024<br /> 
                                        </span>
                                        <a href="https://arxiv.org/abs/2403.08282">[Paper]</a>
                                      </li>
                                      <br>
                                      <li>
                                        <span class="text-primary"><b>Efficient Domain Adaptation via Generative Prior for 3D Infant Pose Estimation</b></span> 
                                        <br />
                                        <span class="text-500">
                                          Zhuoran Zhou, Zhongyu Jiang, <b>Wenhao Chai</b>, Cheng-Yen Yang, Lei Li<sup>&Dagger;</sup>, Jenq-Neng Hwang <br /> 
                                          Winter Conference on Applications of Computer Vision Workshop (WACVW), 2024<br /> 
                                        </span>
                                        <a href="https://zhyjiang.github.io/ZeDO-proj/#infantZeDO">[Website]</a>
                                        <a href="https://arxiv.org/abs/2311.12043">[Paper]</a>
                                        <a href="https://github.com/ipl-uw/ZeDO-Release">[Code]</a>
                                      </li>
                                      <br>
                                      <li>
                                        <span class="text-primary"><b>MPM: A Unified 2D-3D Human Pose Representation via Masked Pose Modeling</b></span> 
                                        <br />
                                        <span class="text-500">
                                          Zhenyu Zhang*, <b>Wenhao Chai</b>*, Zhongyu Jiang, Tian Ye, Mingli Song, Jenq-Neng Hwang, Gaoang Wang<sup>&Dagger;</sup> <br /> 
                                          Chinese Conference on Pattern Recognition and Computer Vision (PRCV), 2024<br /> 
                                        </span>
                                        <a href="https://arxiv.org/abs/2306.17201">[Paper]</a>
                                        <a href="https://github.com/vvirgooo2/MPM">[Code]</a>
                                      </li>
                                      <br>
                                      <li>
                                        <span class="text-primary"><b>Chasing Consistency in Text-to-3D Generation from a Single Image</b></span> 
                                        <br />
                                        <span class="text-500">
                                          Yichen Ouyang, <b>Wenhao Chai</b>, Jiayi Ye, Dapeng Tao, Yibing Zhan, Gaoang Wang<sup>&Dagger;</sup> <br /> 
                                          ACM International Conference on Multimedia in Asia (ACM MM Asia), 2024<br /> 
                                        </span>
                                        <a href="https://arxiv.org/abs/2309.03599">[Paper]</a>
                                      </li>
                                      <br>
                                      <li>
                                        <span class="text-primary"><b>Boosting Online 3D Multi-Object Tracking through Camera-Radar Cross Check</b></span> 
                                        <br />
                                        <span class="text-500">
                                          Sheng-Yao Kuan, Jen-Hao Cheng, Hsiang-Wei Huang, <b>Wenhao Chai</b>, Cheng-Yen Yang, Hugo Latapie, Gaowen Liu, Bing-Fei Wu, Jenq-Neng Hwang<sup>&Dagger;</sup> <br /> 
                                          Intelligent Vehicles Symposium (IV), 2024<br /> 
                                        </span>
                                        <a href="https://ieeexplore.ieee.org/abstract/document/10588514/">[Paper]</a>
                                      </li>
                                      
                                </ol>
                                <br>
                                <h4 class="mb-4 text-uppercase">Journal</h4>
                                <ol>
                                  <li>
                                    <span class="text-primary"><b>Random bridge generator as a platform for developing computer vision-based structural inspection algorithms</b></span> 
                                    <br />
                                    <span class="text-500">
                                      Haojia Cheng*, <b>Wenhao Chai</b>*, Jiabao Hu*, Wenhao Ruan*, Mingyu Shi, Hyunjun Kim, Yifan Cao, Yasutaka Narazaki<sup>&Dagger;</sup><br />
                                      Journal of Infrastructure Intelligence and Resilience<br /> 
                                    </span>
                                    <a href="https://www.sciencedirect.com/science/article/pii/S2772991524000173">[Paper]</a>
                                  </li>
                                  <br>
                                  <li>
                                    <span class="text-primary"><b>Unsupervised Domain Adaptation Approach for Vision-based Semantic Understanding of Bridge Inspection Scenes without Manual Annotations</b></span> 
                                    <br />
                                    <span class="text-500">
                                      Yasutaka Narazaki<sup>&Dagger;</sup>, Wendong Pang, Gaoang Wang, <b>Wenhao Chai</b> <br /> 
                                      ACSE Journal of Bridge Engineering<br /> 
                                    </span>
                                    <a href="https://ascelibrary.org/doi/abs/10.1061/JBENF2.BEENG-6490">[Paper]</a>
                                  </li>
                                </ol>


                                <br>
                                <h4 class="mb-4 text-uppercase">Preprint</h4>
                                <ol>
                                    <li>
                                      <span class="text-primary"><b>AuroraCap: Efficient, Performant Video Detailed Captioning and a New Benchmark</b></span> 
                                      <br />
                                      <span class="text-500">
                                      <u>Wenhao Chai</u>*<sup>&dagger;</sup>, Enxin Song*, Yilun Du, Chenlin Meng, Vashisht Madhavan, Omer Bar-Tal, Jeng-Neng Hwang, Saining Xie, Christopher D. Manning <br /> 
                                      arXiv preprint, 2024<br />   
                                      </span>
                                      <a href="https://rese1f.github.io/aurora-web/">[Website]</a>
                                      <a href="https://arxiv.org/abs/2410.03051">[Paper]</a>
                                      <a href="https://huggingface.co/collections/wchai/auroracap-66d117ffe13bedda96702013">[Model]</a>
                                      <a href="https://huggingface.co/datasets/wchai/Video-Detailed-Caption">[Benchmark]</a>
                                      <a href="https://huggingface.co/datasets/wchai/AuroraCap-trainset">[Dataset]</a>
                                      <a href="https://github.com/rese1f/aurora">[Code]</a>
                                  </li>
                                  <br>
                                  <li>
                                    <span class="text-primary"><b>MovieChat+: Question-aware Sparse Memory for Long Video Question Answering</b></span> 
                                    <br />
                                    <span class="text-500">
                                      Enxin Song*, <b>Wenhao Chai</b>*<sup>&dagger;</sup>, Tian Ye, Jenq-Neng Hwang, Xi Li, Gaoang Wang<sup>&Dagger;</sup> <br /> 
                                      arXiv Preprint.<br /> 
                                    </span>
                                    <a href="https://rese1f.github.io/MovieChat/">[Website]</a>
                                    <a href="https://arxiv.org/abs/2404.17176">[Paper]</a>
                                    <a href="https://www.youtube.com/watch?v=rNzCzflVqBU">[Video]</a>
                                    <a href="https://huggingface.co/datasets/Enxin/MovieChat-1K-test">[Dataset]</a>
                                    <a href="https://github.com/rese1f/MovieChat">[Code]</a>
                                  </li>
                                  <br>
                                  <li>
                                    <span class="text-primary"><b>CityCraft: A Real Crafter for 3D City Generation</b></span> 
                                    <br />
                                    <span class="text-500">
                                      Jie Deng*, <b>Wenhao Chai*</b>, Junsheng Huang*, Zhonghan Zhao*, Qixuan Huang, Mingyan Gao, Jianshu Guo, Shengyu Hao, Wenhao Hu, Jenq-Neng Hwang, Xi Li, Gaoang Wang<sup>&Dagger;</sup> <br /> 
                                      arXiv Preprint.<br /> 
                                    </span>
                                    <a href="https://arxiv.org/abs/2406.04983">[Paper]</a>
                                    <a href="https://github.com/djFatNerd/CityCraft">[Code]</a>
                                  </li>
                                  <br>
                                  <li>
                                    <span class="text-primary"><b>PAD: Personalized Alignment at Decoding-Time</b></span> 
                                    <br />
                                    <span class="text-500">
                                      Ruizhe Chen*, Xiaotian Zhang*, Meng Luo*, <b>Wenhao Chai*</b>, Zuozhu Liu<sup>&Dagger;</sup> <br /> 
                                      arXiv Preprint.<br /> 
                                    </span>
                                    <a href="https://arxiv.org/abs/2410.04070">[Paper]</a>
                                  </li>
                                  <br>
                                  <li>
                                    <span class="text-primary"><b>Exploring Learning-based Motion Models in Multi-Object Tracking</b></span> 
                                    <br />
                                    <span class="text-500">
                                      Hsiang-Wei Huang, Cheng-Yen Yang, <b>Wenhao Chai</b>, Zhongyu Jiang, Jenq-Neng Hwang<sup>&Dagger;</sup> <br /> 
                                      arXiv Preprint.<br /> 
                                    </span>
                                    <a href="https://arxiv.org/abs/2403.10826">[Paper]</a>
                                  </li>
                                  <br>
                                  <li>
                                    <span class="text-primary"><b>VersaT2I: Improving Text-to-Image Models with Versatile Reward</b></span> 
                                    <br />
                                    <span class="text-500">
                                      Jianshu Guo*, <b>Wenhao Chai</b>*<sup>&dagger;</sup>, Jie Deng*, Hsiang-Wei Huang, Tian Ye, Yichen Xu, Jiawei Zhang, Jenq-Neng Hwang, Gaoang Wang<sup>&Dagger;</sup> <br /> 
                                      arXiv Preprint.<br /> 
                                    </span>
                                    <a href="https://arxiv.org/abs/2403.18493">[Paper]</a>
                                  </li>
                                  <br>
                                  <li>
                                    <span class="text-primary"><b>Do We Really Need a Complex Agent System? Distill Embodied Agent into a Single Model</b></span> 
                                    <br />
                                    <span class="text-500">
                                      Zhonghan Zhao, Ke Ma, <b>Wenhao Chai</b><sup>&dagger;</sup>, Xuan Wang, Kewei Chen, Dongxu Guo, Yanting Zhang, Hongwei Wang, Gaoang Wang<sup>&Dagger;</sup> <br /> 
                                      arXiv Preprint.<br /> 
                                    </span>
                                    <a href="https://arxiv.org/abs/2404.04619">[Paper]</a>
                                  </li>
                                  <br>
                                  <li>
                                    <span class="text-primary"><b>AGLLDiff: Guiding Diffusion Models Towards Unsupervised Training-free Real-world Low-light Image Enhancement</b></span> 
                                    <br />
                                    <span class="text-500">
                                      Yunlong Lin, Tian Ye, Sixiang Chen, Zhenqi Fu, Yingying Wang, <b>Wenhao Chai</b>, Zhaohu Xing, Lei Zhu, Xinghao Ding<sup>&Dagger;</sup> <br /> 
                                      arXiv Preprint.<br /> 
                                    </span>
                                    <a href="https://arxiv.org/abs/2407.14900">[Paper]</a>
                                    <a href="https://aglldiff.github.io/">[Website]</a>
                                  </li>
                                  <br>
                                  <li>
                                    <span class="text-primary"><b>MonoTAKD: Teaching Assistant Knowledge Distillation for Monocular 3D Object Detection</b></span> 
                                    <br />
                                    <span class="text-500">
                                      Hou-I Liu, Christine Wu, Jen-Hao Cheng, <b>Wenhao Chai</b>, Shian-Yun Wang, Gaowen Liu, Jenq-Neng Hwang, Hong-Han Shuai, Wen-Huang Cheng<sup>&Dagger;</sup> <br /> 
                                      arXiv Preprint.<br /> 
                                    </span>
                                    <a href="https://arxiv.org/abs/2404.04910">[Paper]</a>
                                  </li>
                                </ol>

                                <br>
                                <h4 class="mb-4 text-uppercase">Technique Report</h4>
                                <ol>
                                  <li>
                                    <span class="text-primary"><b>NTIRE 2024 Image Shadow Removal Challenge Report</b></span> 
                                    <br />
                                    <span class="text-500">
                                      Computer Vision and Pattern Recognition Workshop (CVPRW), 2024<br />
                                    </span>
                                    <a href="https://openaccess.thecvf.com/content/CVPR2024W/NTIRE/papers/Vasluianu_NTIRE_2024_Image_Shadow_Removal_Challenge_Report_CVPRW_2024_paper.pdf">[Report]</a>
                                    <a href="https://cvlai.net/ntire/2024/">[Challenge Website]</a>
                                  </li>
                                </ol>


                                <span id="2023"></span>
                            </div>

                            <div class="mb-8">
                                <h2 class="mb-4 text-uppercase">2023</h2>
                                
                                <h4 class="mb-4 text-uppercase">Conference</h4>
                                <ol>
                                  <li>
                                    <span class="text-primary"><b>StableVideo: Text-driven Consistency-aware Diffusion Video Editing</b></span> 
                                    <br />
                                    <span class="text-500">
                                      <b>Wenhao Chai</b>, Xun Guo<sup>&Dagger;</sup>, Gaoang Wang, Yan Lu <br /> 
                                      International Conference on Computer Vision (ICCV), 2023<br /> 
                                    </span>
                                    <a href="https://rese1f.github.io/StableVideo/">[Website]</a>
                                    <a href="https://arxiv.org/abs/2308.09592">[Paper]</a>
                                    <a href="https://www.youtube.com/watch?v=Q3-GT8du7Mo">[Video]</a>
                                    <a href="https://huggingface.co/spaces/wchai/StableVideo">[Demo]</a>
                                    <a href="https://github.com/rese1f/StableVideo">[Code]</a>
                                  </li>
                                  <br>
                                  <li>
                                    <span class="text-primary"><b>Global Adaptation meets Local Generalization: Unsupervised Domain Adaptation for 3D Human Pose Estimation</b></span> 
                                    <br />
                                    <span class="text-500">
                                      <b>Wenhao Chai</b>, Zhongyu Jiang, Jenq-Neng Hwang, Gaoang Wang<sup>&Dagger;</sup> <br /> 
                                      International Conference on Computer Vision (ICCV), 2023<br /> 
                                    </span>
                                    <a href="https://arxiv.org/abs/2303.16456">[Paper]</a>
                                    <a href="https://github.com/rese1f/PoseDA">[Code]</a>
                                  </li>
                                  <br>
                                  <li>
                                    <span class="text-primary"><b>Sequential Affinity Learning for Video Restoration</b></span> 
                                    <br />
                                    <span class="text-500">
                                      Tian Ye, Sixiang Chen, Yun Liu, <b>Wenhao Chai</b>, Jinbin Bai, Wenbin Zou, Yunchen Zhang, Jiang Mingchao, Erkang Chen<sup>&Dagger;</sup>, Chenghao Xue <br /> 
                                      ACM International Conference on Multimedia (ACM MM), 2023<br /> 
                                    </span>
                                    <a href="https://dl.acm.org/doi/10.1145/3581783.3611742">[Paper]</a>
                                    <a href="https://github.com/Owen718/SALN">[Code]</a>
                                  </li>
                                  <br>
                                  <li>
                                    <span class="text-primary"><b>PoSynDA: Multi-Hypothesis Pose Synthesis Domain Adaptation for Enhanced 3D Human Pose Estimation</b></span> 
                                    <br />
                                    <span class="text-500">
                                      Hanbing Liu, Jun-Yan He, Zhi-Qi Cheng, Wangmeng Xiang, Qize Yang, <b>Wenhao Chai</b>, Gaoang Wang, Xu Bao, Bin Luo, Yifeng Geng, Xuansong Xie<sup>&Dagger;</sup> <br /> 
                                      ACM International Conference on Multimedia (ACM MM), 2023<br /> 
                                    </span>
                                    <a href="https://arxiv.org/abs/2308.09678">[Paper]</a>
                                    <a href="https://github.com/hbing-l/PoSynDA">[Code]</a>
                                  </li>
                                  <br>
                                  <li>
                                    <span class="text-primary"><b>Five A+ Network: You Only Need 9K Parameters for Underwater Image Enhancement</b></span> 
                                    <br />
                                    <span class="text-500">
                                      Jingxia Jiang, Tian Ye, Jinbin Bai, Sixiang Chen, <b>Wenhao Chai</b>, Jun Shi, Yun Liu, Erkang Chen<sup>&Dagger;</sup> <br /> 
                                      British Machine Vision Conference (BMVC), 2023<br /> 
                                    </span>
                                    <a href="https://arxiv.org/abs/2305.08824">[Paper]</a>
                                    <a href="https://github.com/Owen718/FiveAPlus-Network">[Code]</a>
                                  </li>
                                  <br>
                                  <li>
                                    <span class="text-primary"><b>Image Reference-guided Fashion Design with Structure-aware Transfer by Diffusion Models</b></span> 
                                    <br />
                                    <span class="text-500">
                                      Shidong Cao*, <b>Wenhao Chai</b>*, Shengyu Hao, Gaoang Wang<sup>&Dagger;</sup> <br /> 
                                      Computer Vision and Pattern Recognition Workshop (CVPRW), 2023<br /> 
                                    </span>
                                    <a href="https://openaccess.thecvf.com/content/CVPR2023W/CVFAD/html/Cao_Image_Reference-Guided_Fashion_Design_With_Structure-Aware_Transfer_by_Diffusion_Models_CVPRW_2023_paper.html">[Paper]</a>
                                    <a href="https://github.com/Rem105-210/DiffFashion">[Code]</a>
                                  </li>
                                  <br>
                                  <li>
                                    <span class="text-primary"><b>Devil in the Number: Towards Robust Multi-modality Data Filter</b></span> 
                                    <br />
                                    <span class="text-500">
                                      Yichen Xu, Zihan Xu, <b>Wenhao Chai</b><sup>&dagger;</sup>, Zhonghan Zhao, Enxin Song, Gaoang Wang<sup>&Dagger;</sup> <br /> 
                                      International Conference on Computer Vision Workshop (ICCVW), 2023<br /> 
                                    </span>
                                    <a href="https://arxiv.org/abs/2309.13770">[Paper]</a>
                                  </li>
                                  <br>
                                  <li>
                                    <span class="text-primary"><b>User-Aware Prefix-Tuning is a Good Learner for Personalized Image Captioning</b></span> 
                                    <br />
                                    <span class="text-500">
                                      Xuan Wang, Guanhong Wang, <b>Wenhao Chai</b>, Jiayu Zhou, Gaoang Wang<sup>&Dagger;</sup> <br /> 
                                      Chinese Conference on Pattern Recognition and Computer Vision (PRCV), 2023<br /> 
                                    </span>
                                  </li>
                                </ol>

                                <br>
                                <h4 class="mb-4 text-uppercase">Journal</h4>
                                <ol>
                                  <li>
                                    <span class="text-primary"><b>DiffFashion: Reference-based Fashion Design with Structure-aware Transfer by Diffusion Models</b></span> 
                                    <br />
                                    <span class="text-500">
                                      Shidong Cao*, <b>Wenhao Chai</b>*, Shengyu Hao, Yanting Zhang, Hangyue Chen, Gaoang Wang<sup>&Dagger;</sup> <br /> 
                                      IEEE Transactions on Multimedia<br /> 
                                    </span>
                                    <a href="https://arxiv.org/abs/2302.06826">[Paper]</a>
                                    <a href="https://github.com/Rem105-210/DiffFashion">[Code]</a>
                                  </li>
                                  <br>
                                  <li>
                                    <span class="text-primary"><b>Deep Learning Methods for Small Molecule Drug Discovery: A Survey</b></span> 
                                    <br />
                                    <span class="text-500">
                                      Wenhao Hu*, Yingying Liu*, Xuanyu Chen, <b>Wenhao Chai</b>, Hangyue Chen, Hongwei Wang, Gaoang Wang<sup>&Dagger;</sup> <br /> 
                                      IEEE Transactions on Artificial Intelligence<br /> 
                                    </span>
                                    <a href="https://arxiv.org/abs/2303.00313">[Paper]</a>
                                  </li>

                                </ol>
                                
                                <br>
                                <h4 class="mb-4 text-uppercase">Preprint</h4>
                                <ol>
                                      <li>
                                        <span class="text-primary"><b>CityGen: Infinite and Controllable 3D City Layout Generation</b></span> 
                                        <br />
                                        <span class="text-500">
                                          Jie Deng*, <b>Wenhao Chai</b>*<sup>&dagger;</sup>, Jianshu Guo*, Qixuan Huang, Wenhao Hu, Jenq-Neng Hwang, Gaoang Wang<sup>&Dagger;</sup> <br /> 
                                          arXiv Preprint.<br /> 
                                        </span>
                                        <a href="https://rese1f.github.io/CityGen/">[Website]</a>
                                        <a href="https://arxiv.org/abs/2312.01508">[Paper]</a>
                                        <a href="https://github.com/rese1f/CityGen">[Code]</a>
                                      </li>
                                      <br>
                                      <li>
                                        <span class="text-primary"><b>UniHPE: Towards Unified Human Pose Estimation via Contrastive Learning</b></span> 
                                        <br />
                                        <span class="text-500">
                                          Zhongyu Jiang, <b>Wenhao Chai</b>, Lei Li, Zhuoran Zhou, Cheng-Yen Yang, Jenq-Neng Hwang<sup>&Dagger;</sup> <br /> 
                                          arXiv Preprint.<br /> 
                                        </span>
                                        <a href="https://arxiv.org/abs/2311.16477">[Paper]</a>
                                      </li>
                                      <br>
                                      <li>
                                        <span class="text-primary"><b>A Survey of Deep Learning in Sports Applications: Perception, Comprehension, and Decision</b></span> 
                                        <br />
                                        <span class="text-500">
                                          Zhonghan Zhao*, <b>Wenhao Chai</b>*, Shengyu Hao, Wenhao Hu, Guanhong Wang, Shidong Cao, Gaoang Wang<sup>&Dagger;</sup>, Mingli Song, Jenq-Neng Hwang <br /> 
                                          arXiv Preprint.<br /> 
                                        </span>
                                        <a href="https://arxiv.org/abs/2307.03353">[Paper]</a>
                                      </li>
                                </ol>
                                <span id="2022"></span>
                            </div>

                            <div class="mb-8">

                                <h2 class="mb-4 text-uppercase">2022</h2>
                                <h4 class="mb-4 text-uppercase">Conference</h4>
                                <ol>
                                <li>
                                    <span class="text-primary"><b>Automatic Spinal Ultrasound Image Segmentation and Deployment for Real-time Spine Volumetric Reconstruction</b></span> 
                                    <br />
                                    <span class="text-500">
                                      Yifan Cao*, Chenghao Tan*, Wenzhuo Qian, <b>Wenhao Chai</b>, Luhang Cui, Wenxuan Yang, Xinben Hu, Yongjian Zhu, Wenhui Zhou<sup>&Dagger;</sup>, Xingfa Shen <br /> 
                                      International Conference on Unmanned Systems (ICUS), 2022<br />
                                      Best Paper Award<br />
                                      
                                    </span>
                                    <a href="https://ieeexplore.ieee.org/document/9987127">[Paper]</a>
                                    <a href="https://github.com/deeper-coder/ICUS-2022">[Code]</a>
                                  </li>
                                  <br>
                                  <li>
                                    <span class="text-primary"><b>Weakly Supervised Two-Stage Training Scheme for Deep Video Fight Detection Model</b></span> 
                                    <br />
                                    <span class="text-500">
                                      Zhenting Qi*, Ruike Zhu*, Zheyu Fu*, <b>Wenhao Chai</b>*, Volodymyr Kindratenko<sup>&Dagger;</sup> <br /> 
                                      International Conference on Tools with Artificial Intelligence (ICTAI), 2022<br /> 
                                    </span>
                                    <a href="https://arxiv.org/abs/2209.11477">[Paper]</a>
                                    <a href="https://github.com/Hepta-Col/VideoFightDetection">[Dataset]</a>
                                </li>
                                </ol>

                                <br>
                                <h4 class="mb-4 text-uppercase">Journal</h4>
                                <ol>
                                  <li>
                                    <span class="text-primary"><b>Deep Vision Multimodal Learning: Methodology, Benchmark, and Trend</b></span> 
                                    <br />
                                    <span class="text-500">
                                      <b>Wenhao Chai</b>, Gaoang Wang<sup>&Dagger;</sup> <br /> 
                                      Applied Sciences<br /> 
                                    </span>
                                    <a href="https://www.mdpi.com/2076-3417/12/13/6588">[Paper]</a>
                                  </li>
                                </ol>
                                  
                            </div>                                                     
                        </div>                        
                    </div>
                </div>
            </section>
            <!-- End of Section -->
        </main>
        <!-- End of Main Content -->


        <!-- Footer -->
        <footer class="footer_p text-white" style="background-image: url(assets/img/logo-small.png)">
            <div class="container d-flex h-100">
                <div class="row flex-grow-1">
                    <div class="col-lg-3 pt-3 ext-l bg-dark text-center text-lg-left">
                        <div class="d-flex flex-column h-100">
                            <div class="pt-5 pt-lg-8 pb-4">
                                <img src="assets/img/logo-small.png" alt="" width="108" class="mb-4" />
                                <p class="mb-4 mt-3 fs--1"><br />
                                Wenhao Chai <br />
                                University of Washington<br />
                                Seattle, WA 98195</p>
        
                                <p class="fs--1">
                                <span class="text-white"><i class="zmdi zmdi-email zmdi-hc-fw mr-1"></i>wchai at uw.edu</span><br />
                                <span class="text-white"><i class="zmdi zmdi-twitter zmdi-hc-fw mr-1"></i><a href="https://twitter.com/re5e1f" target="_blank" class="text-white">@re5e1f</a></span></p>
                            </div>
    
                            <!-- <ul class="mt-4 mt-lg-auto mb-5 mb-lg-0 list-unstyled list-inline">
                                <li class="mr-3 list-inline-item">
                                    <a href="https://twitter.com/re5e1f" target="_blank">
                                        <i class="zmdi zmdi-twitter text-white"></i>
                                    </a>
                                </li>
                            </ul> -->
                        </div>
                    </div>

                    <div class="col d-flex flex-column mb-2 mt-3 pl-lg-7">
                        <div class="row pt-5 pt-lg-8 pb-4 pb-lg-6">
                            <div class="col-6 col-lg-3">
                                <h6 class="mb-1 mb-lg-4 text-uppercase">Publications</h6>
                                <ul class="pt-2 mb-5 fw-light list-unstyled">
                                    <li class="my-1"><a href="publication_topic.html" class="text-white">By Topic</a></li>
                                    <li class="my-1"><a href="publication_year.html" class="text-white">By Year</a></li>
                                </ul>
                            </div>
                            <div class="col-6 col-lg-3">
                                <h6 class="mb-1 mb-lg-4 text-uppercase">About</h6>
                                <ul class="pt-2 mb-5 fw-light list-unstyled">
                                    <li class="my-1"><a href="research.html" class="text-white">Our Research</a></li>
                                    <!-- <li class="my-1"><a href="team.html" class="text-white">Team</a></li>
                                    <li class="my-1"><a href="careers.html" class="text-white">Join Us</a></li> -->
                                </ul>
                            </div>
                            <div class="col-6 col-lg-3">
                                <h6 class="mb-1 mb-lg-4 text-uppercase">Open Source</h6>
                                <ul class="pt-2 mb-5 fw-light list-unstyled">
                                    <li class="my-1"><a href="downloads.html" class="text-white">Code and Datasets</a></li>
                                </ul>
                            </div>
                        </div>

                        <div class="mt-auto d-flex justify-content-between">
                            <span class="fs--3 fs-lg--2">&copy; Wenhao Chai, 2024</span>
                        </div>
                    </div>
                </div>
            </div>
        </footer>
        <!-- End of Footer -->

        <!-- Top Button -->
        <a id="back-to-top" href="publication_year.html#" class="btn btn-light btn-lg back-to-top" role="button"><i class="fas fa-chevron-up"></i></a>
        <!-- End of Top Button -->


        <!-- Core Javascripts -->
        <script src="assets/vendor/jquery/dist/jquery.min.js"></script>
        <script src="assets/vendor/popper.js/dist/umd/popper.min.js"></script>
        <script src="assets/vendor/bootstrap/dist/js/bootstrap.min.js"></script>
        <script src="assets/vendor/typed.js/lib/typed.min.js"></script>

        <!-- Vendor Javascripts -->
        <script src="assets/vendor/rellax/rellax.min.js"></script>
        <script src="assets/vendor/sticky-kit/dist/sticky-kit.min.js"></script>
        <script src="assets/vendor/imagesloaded/imagesloaded.pkgd.min.js"></script>
        <script src="assets/vendor/isotope-layout/dist/isotope.pkgd.min.js"></script>
        <script src="assets/vendor/isotope-packery/packery-mode.pkgd.min.js"></script>
        <script src="assets/vendor/aos/dist/aos.js"></script>

        <!-- Theme Javascripts -->
        <script src="assets/js/theme.js"></script>
        <script src="assets/js/top.js"></script>
    </body>
</html>